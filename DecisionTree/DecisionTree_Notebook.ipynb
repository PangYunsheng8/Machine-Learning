{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID3算法的缺点：\n",
    "1) 用信息增益选择最优特征时，其对取值较多的特征有偏好，造成特征选择不准确  \n",
    "2) 无法处理缺失值  \n",
    "3) 无法处理连续值  \n",
    "4) 没有剪枝，容易造成过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C4.5算法对ID3的改进：\n",
    "C4.5对以上ID3的四个缺点分别做了改进如下:  \n",
    "1) 把特征选择的标准由信息增益改为了信息增益率  \n",
    "2) C4.5增加了对连续值的处理，处理的方法是连续特征离散化，具体为：C4.5将连续特征的m个值排序，两两取均值得到m-1个划分点，对于每个划分点，分别计算以该点为二元分类点时的信息增益，选择信息增益最大的点作为该连续特征的二元离散分类点。再以该点为内部结点细分为两个子节点，小于该划分点的样本划分到左结点，大于该划分点的样本划分到右结点。  \n",
    "3) C4.5对于缺失值的处理分为两个问题，第一个是对于缺失部分数据的特征如何计算其信息增益。简单来说C4.5只计算不缺失特征值的那部分数据的信息增益，再对该信息增益乘以一个系数(该系数可以为不缺失数据占总数居的比重)。第二个是选定了划分特征后，如何处理一个缺失该特征的值的样本。  \n",
    "4) C4.5引入了正则化系数进行初步的剪枝，以防止过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C4.5算法的缺点：\n",
    "1) C4.5为多叉树，计算机在处理多叉树时效率远低于二叉树  \n",
    "2) C4.5只能用于分类，不能用于回归  \n",
    "3) C4.5使用了熵模型，其中包含大量的耗时的对数运算  \n",
    "4) C4.5的剪枝过程还有优化的空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART,分类与回归树，既可以用于分类任务也可以用于回归任务。CART同样由特征选择，决策树生成和决策树剪枝组成。CART假设决策树是二叉树，内部结点\n",
    "特征的取值是“是”和“否”，左分支取值为“是”，右分支取值为“否”。CART对回归树用平方误差最小化准则，对分类树用基尼指数(Gini index)最小化准则，进行特征选择，生成二叉树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CART算法对C4.5的改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) CART为二叉树，计算效率更高  \n",
    "2) CART既能用来分类也能用于回归  \n",
    "3) CART将特征选择的标准由C4.5的信息增益率改为了基尼指数，保留了熵模型的特点，但同时避免了大量的对数运算  \n",
    "4) CART采用了后剪枝和交叉验证的方法对模型进行剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 什么是分类树，什么是回归树？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "一般来说，对于连续变量的处理用回归树，对于离散变量的处理用分类树，但是离散或连续变量包含两种情况，第一种是数据中包含离散或连续变量，二是模\n",
    "型的输出为离散或连续变量。\n",
    "在决策树中，如果模型的输出为离散值，那么这棵树一定是分类树，如果模型的输出为连续值，那么这棵树一定是回归树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特征的重复使用问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于ID3，它只能处理离散特征，当一个离散特征被当作划分结点后，再后续生成树的过程中将不会再使用该特征。  \n",
    "对于C4.5，离散特征同样只能被处理一次，但是连续特征可以被处理多次。  \n",
    "对于CART，离散特征和连续特征都可以被处理多次。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 决策树算法的优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1）简单直观，生成的决策树很直观\n",
    "\n",
    "2）基本不需要预处理，不需要提前归一化，处理缺失值\n",
    "\n",
    "3）使用决策树预测的代价是O(log2m), m为样本数\n",
    "\n",
    "4）既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值\n",
    "\n",
    "5）可以处理多维度输出的分类问题\n",
    "\n",
    "6）相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释\n",
    "\n",
    "7）可以交叉验证的剪枝来选择模型，从而提高泛化能力\n",
    "\n",
    "8）对于异常点的容错能力好，健壮性高"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1）决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进\n",
    "\n",
    "2）决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决\n",
    "\n",
    "3）寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善\n",
    "\n",
    "4）有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决\n",
    "\n",
    "5）如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 决策树概要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **决策树是一个非线性模型，可以用来分类和回归**  \n",
    "+ **从可能的决策树中直接选取最优的决策树是NP难问题，一般采用启发式的方法选取次优的决策树**  \n",
    "+ **决策树的生成过程中，通常选取信息增益最大，信息增益率最大或者基尼指数最小的特征作为划分特征**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datamining]",
   "language": "python",
   "name": "conda-env-datamining-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
